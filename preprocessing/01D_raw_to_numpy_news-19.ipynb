{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle as cPickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from file_util import create_folder\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "    \n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_dataset        = 'news-19_paragraph_swap-random-1m'\n",
    "# name_dataset        = 'headline_swap_news_v2'\n",
    "# name_dataset        = 'headline_swap_news_v2.5'\n",
    "# name_dataset        = 'headline_swap_news_v2.5_mf8'\n",
    "name_dataset        = 'headline'\n",
    "path_raw_data       = '../data/raw/' + name_dataset + '/'\n",
    "path_processed_data = '../data/' + name_dataset + '/whole/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(path_raw_data)\n",
    "create_folder(path_processed_data + '/train')\n",
    "create_folder(path_processed_data + '/dev')\n",
    "create_folder(path_processed_data + '/test')\n",
    "create_folder(path_processed_data + '/debug')\n",
    "create_folder(path_processed_data + '/real_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/headline/\n"
     ]
    }
   ],
   "source": [
    "print(path_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/headline/whole/\n"
     ]
    }
   ],
   "source": [
    "print (path_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get term-/document-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:23:35.656687\n",
      "736\n",
      "13:23:36.379752\n",
      "voca size : 45311\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + 'train.csv', encoding=\"utf8\"), delimiter='\\t')\n",
    "\n",
    "tkn2tf = {}\n",
    "len_heads = [] #1\n",
    "len_paras = [] #2\n",
    "cnt_paras = [] #3\n",
    "len_bodys = [] #4\n",
    "\n",
    "# csv data: 0:id, 1:head, 2:body, 3:label\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if (n+1) % 100000 == 0: print (n+1),\n",
    "    \n",
    "    head = row[0].lower().strip()\n",
    "    \n",
    "    for tkn in head.split():\n",
    "        if tkn in tkn2tf: tkn2tf[tkn] += 1\n",
    "        else: tkn2tf[tkn] = 1\n",
    "    len_heads.append(len(head.split())) #1\n",
    "    \n",
    "    body = row[0].lower().strip()\n",
    "    tkn_para = []\n",
    "    for para in body.split('<eop>'):\n",
    "        if para and para != ' ':\n",
    "            _para = para + '<eop>'\n",
    "            len_para = len(_para.split())\n",
    "            len_paras.append(len_para) #2\n",
    "            tkn_para.append(_para)\n",
    "    cnt_paras.append(len(tkn_para)) #3\n",
    "    \n",
    "    body_split = []\n",
    "    for tkn in body.split():\n",
    "        if tkn in tkn2tf: tkn2tf[tkn] += 1\n",
    "        else: tkn2tf[tkn] = 1\n",
    "        body_split.append(tkn)\n",
    "    len_bodys.append(len(body_split)) #4\n",
    "            \n",
    "print (n+1), 'Done'\n",
    "print (datetime.datetime.now().time())\n",
    "print ('voca size :', len(tkn2tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45311it [00:00, 567913.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45313 No problem\n",
      "Show top-10 tkn:\n",
      "<eos> : 42450\n",
      "। : 36968\n",
      ", : 22574\n",
      "না : 7024\n",
      "করে : 6546\n",
      "এই : 5026\n",
      "! : 3764\n",
      "? : 3668\n",
      "থেকে : 3442\n",
      "’ : 3216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_token = sorted(tkn2tf.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tkn2idx = {}\n",
    "for idx, (tkn, _) in tqdm(enumerate(sorted_token)):\n",
    "    tkn2idx[tkn] = idx + 2\n",
    "tkn2idx['<unk>'] = 1\n",
    "tkn2idx[''] = 0\n",
    "if len(tkn2idx) == len(tkn2tf)+2:\n",
    "    print (len(tkn2idx), 'No problem')\n",
    "print \n",
    "\n",
    "print ('Show top-10 tkn:')\n",
    "for tkn, freq in sorted_token[:10]:\n",
    "    print (tkn,':',freq)\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dic_mincut.txt\n",
    "with open(\"Output.txt\", \"w\", encoding=\"utf8\") as txt_file:\n",
    "    for tkn, freq in sorted_token:\n",
    "        txt_file.write(tkn + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.txt', 'w', encoding='utf8') as f:\n",
    "    for key in tkn2idx.keys():\n",
    "        f.write(key+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca size : 3308\n"
     ]
    }
   ],
   "source": [
    "tkn2tf_mincut5 = {}\n",
    "for tkn, tf in tkn2tf.items():\n",
    "    if tf < 30:\n",
    "        continue\n",
    "    tkn2tf_mincut5[tkn] = tf\n",
    "print ('voca size :', len(tkn2tf_mincut5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2tf_mincut5['<EOS>'] = tkn2tf_mincut5['<eos>']\n",
    "tkn2tf_mincut5['<EOP>'] = tkn2tf_mincut5['<eop>']\n",
    "\n",
    "del tkn2tf_mincut5['<eos>']\n",
    "del tkn2tf_mincut5['<eop>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_voca = sorted(tkn2tf_mincut5.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing word <EOP>\n",
      "existing word <EOS>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3310"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_voca_mincut = []\n",
    "list_voca_mincut.append('')   # PAD\n",
    "list_voca_mincut.append('<UNK>')   # UNK\n",
    "list_voca_mincut.append('<EOS>')   # EOS\n",
    "list_voca_mincut.append('<EOP>')   # EOP\n",
    "\n",
    "for word, idx in sorted_voca:\n",
    "    if word=='<UNK>' or word=='<EOP>' or word=='<EOS>':\n",
    "        print(\"existing word\", word)\n",
    "        continue\n",
    "    else:\n",
    "        list_voca_mincut.append(word)\n",
    "        \n",
    "len(list_voca_mincut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.txt', 'w', encoding='utf8') as f:\n",
    "    for i in range(len(list_voca_mincut)):\n",
    "        f.write(list_voca_mincut[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_voca = {}\n",
    "for voca in list_voca_mincut:\n",
    "    dic_voca[voca] = len(dic_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(dic_voca[''], dic_voca['<UNK>'], dic_voca['<EOS>'], dic_voca['<EOP>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.pkl', 'wb') as f:\n",
    "    cPickle.dump(dic_voca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read voca from dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3310"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_voca = []\n",
    "with open(path_processed_data + 'dic_mincutN.txt', 'r', encoding='utf8') as f:\n",
    "    list_voca = f.readlines()\n",
    "    list_voca = [x.strip() for x in list_voca]\n",
    "\n",
    "dic_voca = {}\n",
    "for voca in list_voca:\n",
    "    dic_voca[voca] = len(dic_voca)\n",
    "\n",
    "len(dic_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dic_voca_lower = copy.deepcopy(dic_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_voca_lower['<eos>'] = dic_voca_lower['<EOS>']\n",
    "dic_voca_lower['<eop>'] = dic_voca_lower['<EOP>']\n",
    "\n",
    "del dic_voca_lower['<EOS>']\n",
    "del dic_voca_lower['<EOP>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3310"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic_voca_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(dic_voca_lower[''], dic_voca_lower['<UNK>'], dic_voca_lower['<eos>'], dic_voca_lower['<eop>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "data= []\n",
    "with open(path_raw_data + 'train.csv', 'r', encoding='utf8') as f:\n",
    "    data_csv = csv.reader(f, delimiter='\\t')\n",
    "    for row in data_csv:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(data):\n",
    "    print(\"mean\", np.average(data))\n",
    "    print(\"std\", np.std(data))\n",
    "    print(\"max\", np.max(data))\n",
    "    print(\"95.xx coverage\", np.average(data) +  2*np.std(data) )\n",
    "    print(\"99.73 coverage\", np.average(data) +  3*np.std(data) )\n",
    "    print(\"99.95 coverage\", np.average(data) +  3.5*np.std(data) )\n",
    "    print(\"99.99 coverage\", np.average(data) +  4*np.std(data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_len\n",
      "mean 523.0951086956521\n",
      "std 354.01989501019807\n",
      "max 3653\n",
      "95.xx coverage 1231.1348987160482\n",
      "99.73 coverage 1585.1547937262462\n",
      "99.95 coverage 1762.1647412313455\n",
      "99.99 coverage 1939.1746887364443\n"
     ]
    }
   ],
   "source": [
    "head = [x[0].strip() for x in data]\n",
    "head_len = [len(x.split()) for x in head]\n",
    "print('head_len')\n",
    "print_info(head_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = [x[0].strip() for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_len\n",
      "mean 523.0951086956521\n",
      "std 354.01989501019807\n",
      "max 3653\n",
      "95.xx coverage 1231.1348987160482\n",
      "99.73 coverage 1585.1547937262462\n",
      "99.95 coverage 1762.1647412313455\n",
      "99.99 coverage 1939.1746887364443\n"
     ]
    }
   ],
   "source": [
    "body_len = [len(x.split()) for x in body ]\n",
    "print('body_len')\n",
    "print_info(body_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_len\n",
      "mean 1.5\n",
      "std 0.5\n",
      "max 2\n",
      "95.xx coverage 2.5\n",
      "99.73 coverage 3.0\n",
      "99.95 coverage 3.25\n",
      "99.99 coverage 3.5\n"
     ]
    }
   ],
   "source": [
    "context_len = [len(x.split('<EOP>')) for x in body]\n",
    "print('context_len')\n",
    "print_info(context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_sentence\n",
      "mean 348.3967391304348\n",
      "std 253.67898133088772\n",
      "max 3366\n",
      "95.xx coverage 855.7547017922102\n",
      "99.73 coverage 1109.433683123098\n",
      "99.95 coverage 1236.2731737885417\n",
      "99.99 coverage 1363.1126644539856\n"
     ]
    }
   ],
   "source": [
    "body_sentence = []\n",
    "for sent in body:\n",
    "    sent = sent.split('<EOP>')\n",
    "    body_sentence.extend(sent)\n",
    "body_len = [ len(x.split()) for x in body_sentence ]    \n",
    "print('body_sentence')\n",
    "print_info(body_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# encode to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_length(data, max_len_t, max_len_b):\n",
    "    data_t, data_b = data\n",
    "    \n",
    "    list_zeros = np.zeros(max_len_b, 'int32').tolist()\n",
    "    fl_data_t = []\n",
    "    for datum in data_t:\n",
    "        try:\n",
    "            datum = list(datum)\n",
    "        except:\n",
    "            pass\n",
    "        _len = len(datum)\n",
    "        if _len >= max_len_t:\n",
    "            fl_data_t.append( datum[:max_len_t] )\n",
    "        else:\n",
    "            fl_data_t.append( datum + list_zeros[:(max_len_t-_len)] )\n",
    "            \n",
    "    fl_data_b = []\n",
    "    for datum in data_b:\n",
    "        try:\n",
    "            datum = list(datum)\n",
    "        except:\n",
    "            pass\n",
    "        _len = len(datum)\n",
    "        if _len >= max_len_b:\n",
    "            fl_data_b.append( datum[:max_len_b] )\n",
    "        else:\n",
    "            fl_data_b.append( datum + list_zeros[:(max_len_b-_len)] )\n",
    "    \n",
    "    np_data_t = np.asarray(fl_data_t, dtype='int32')\n",
    "    np_data_b = np.asarray(fl_data_b, dtype='int32')\n",
    "    \n",
    "    data = [np_data_t, np_data_b]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:36.577271\n",
      "736 Done\n",
      "2021-02-28T13:26:37.154727\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + 'train.csv', 'r', encoding='utf8'), delimiter='\\t')\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "ids = []\n",
    "heads = []\n",
    "bodys = []\n",
    "labels = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    \n",
    "#     if n <  3000000:\n",
    "#         continue\n",
    "\n",
    "#     if n >=  3000000:\n",
    "#         continue\n",
    "        \n",
    "                \n",
    "    if (n+1) % 10000 == 0: print (n+1,)\n",
    "    \n",
    "    ids.append(row[0])\n",
    "    labels.append((row[0]))\n",
    "    \n",
    "    head = []\n",
    "    for tkn in row[0].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            head.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            head.append(1)\n",
    "            \n",
    "    heads.append(head)\n",
    "    \n",
    "    body = []\n",
    "    for tkn in row[0].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            body.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            body.append(1)\n",
    "            \n",
    "    bodys.append(body)\n",
    "    \n",
    "print (n+1, 'Done')\n",
    "print (datetime.datetime.now().isoformat()) # ~5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:38.739488\n",
      "2021-02-28T13:26:38.959934\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "[np_heads, np_bodys] = fit_length([heads, bodys], 25, 2100)\n",
    "print (datetime.datetime.now().isoformat()) # ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:40.262418\n",
      "2021-02-28T13:26:41.045323\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/train/train_title.npy'\n",
    "np.save(t_trainpath, np_heads)\n",
    "b_trainpath = path_processed_data + '/train/train_body.npy'\n",
    "np.save(b_trainpath, np_bodys)\n",
    "l_trainpath = path_processed_data + '/train/train_label.npy'\n",
    "np.save(l_trainpath, labels)\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:42.676961\n",
      "65 Done\n",
      "2021-02-28T13:26:42.766721\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + 'dev.csv', 'r', encoding='utf8'), delimiter='\\t')\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "ids_dev = []\n",
    "heads_dev = []\n",
    "bodys_dev = []\n",
    "labels_dev = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if (n+1) % 10000 == 0: print (n+1,)\n",
    "    \n",
    "    ids_dev.append(row[0])\n",
    "    labels_dev.append((row[0]))\n",
    "    \n",
    "    head = []\n",
    "    for tkn in row[0].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            head.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            head.append(1)\n",
    "    heads_dev.append(head)\n",
    "    \n",
    "    body = []\n",
    "    for tkn in row[0].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            body.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            body.append(1)\n",
    "    bodys_dev.append(body)\n",
    "    \n",
    "print (n+1, 'Done')\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:44.534480\n",
      "2021-02-28T13:26:44.556386\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "[np_heads_dev, np_bodys_dev] = fit_length([heads_dev, bodys_dev], 25, 2100)\n",
    "print (datetime.datetime.now().isoformat()) # ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:46.886249\n",
      "2021-02-28T13:26:46.900211\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/dev/dev_title.npy'\n",
    "np.save(t_trainpath, np_heads_dev)\n",
    "b_trainpath = path_processed_data + '/dev/dev_body.npy'\n",
    "np.save(b_trainpath, np_bodys_dev)\n",
    "l_trainpath = path_processed_data + '/dev/dev_label.npy'\n",
    "np.save(l_trainpath, labels_dev)\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:49.157176\n",
      "65 Done\n",
      "2021-02-28T13:26:49.242945\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + '/test.csv', 'r', encoding='utf8'), delimiter='\\t')\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "ids_dev = []\n",
    "heads_dev = []\n",
    "bodys_dev = []\n",
    "labels_dev = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if (n+1) % 10000 == 0: print (n+1,)\n",
    "    \n",
    "    ids_dev.append(row[0])\n",
    "    labels_dev.append(row[0])\n",
    "    \n",
    "    head = []\n",
    "    for tkn in row[0].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            head.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            head.append(1)\n",
    "    heads_dev.append(head)\n",
    "    \n",
    "    body = []\n",
    "    for tkn in row[0].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            body.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            body.append(1)\n",
    "    bodys_dev.append(body)\n",
    "    \n",
    "print (n+1, 'Done')\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:53.110848\n",
      "2021-02-28T13:26:53.137775\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "[np_heads_dev, np_bodys_dev] = fit_length([heads_dev, bodys_dev], 25, 2100)\n",
    "print (datetime.datetime.now().isoformat()) # ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:57.054703\n",
      "2021-02-28T13:26:57.125513\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/test/test_title.npy'\n",
    "np.save(t_trainpath, np_heads_dev)\n",
    "b_trainpath = path_processed_data + '/test/test_body.npy'\n",
    "np.save(b_trainpath, np_bodys_dev)\n",
    "l_trainpath = path_processed_data + '/test/test_label.npy'\n",
    "np.save(l_trainpath, labels_dev)\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debugset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-28T13:26:59.560404\n",
      "2021-02-28T13:26:59.593314\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/debug/debug_title.npy'\n",
    "np.save(t_trainpath, np_heads_dev[:200])\n",
    "b_trainpath = path_processed_data + '/debug/debug_body.npy'\n",
    "np.save(b_trainpath, np_bodys_dev[:200])\n",
    "l_trainpath = path_processed_data + '/debug/debug_label.npy'\n",
    "np.save(l_trainpath, labels_dev[:200])\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.txt', encoding='utf8') as f:\n",
    "    test_list_voca = f.readlines()\n",
    "    test_list_voca = [x.strip() for x in test_list_voca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Vocab(test_list_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> <UNK> <UNK> নির্বাচনী প্রচারণা চান বাংলাদেশের <UNK> শেখ <UNK> রহমানের ছোট ভাই শেখ আবু <UNK> ছেলে শেখ <UNK> <UNK> সন্তান শেখ <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(tt.index2sent(np_heads_dev[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
