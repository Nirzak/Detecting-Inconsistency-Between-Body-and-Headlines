{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle as cPickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from file_util import create_folder\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "    \n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_dataset        = 'news-19_paragraph_swap-random-1m'\n",
    "# name_dataset        = 'headline_swap_news_v2'\n",
    "# name_dataset        = 'headline_swap_news_v2.5'\n",
    "# name_dataset        = 'headline_swap_news_v2.5_mf8'\n",
    "name_dataset        = 'headline'\n",
    "path_raw_data       = '../data/raw/' + name_dataset + '/'\n",
    "path_processed_data = '../data/' + name_dataset + '/whole/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(path_raw_data)\n",
    "create_folder(path_processed_data + '/train')\n",
    "create_folder(path_processed_data + '/dev')\n",
    "create_folder(path_processed_data + '/test')\n",
    "create_folder(path_processed_data + '/debug')\n",
    "create_folder(path_processed_data + '/real_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/headline/\n"
     ]
    }
   ],
   "source": [
    "print(path_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/headline/whole/\n"
     ]
    }
   ],
   "source": [
    "print (path_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get term-/document-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:35:40.015747\n",
      "736\n",
      "17:35:40.895437\n",
      "voca size : 44031\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + 'train.csv', encoding='utf-8'))\n",
    "\n",
    "tkn2tf = {}\n",
    "len_heads = [] #1\n",
    "len_paras = [] #2\n",
    "cnt_paras = [] #3\n",
    "len_bodys = [] #4\n",
    "\n",
    "# csv data: 0:id, 1:head, 2:body, 3:label\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if (n+1) % 100000 == 0: print (n+1),\n",
    "    \n",
    "    head = row[1].lower().strip()\n",
    "    \n",
    "    for tkn in head.split():\n",
    "        if tkn in tkn2tf: tkn2tf[tkn] += 1\n",
    "        else: tkn2tf[tkn] = 1\n",
    "    len_heads.append(len(head.split())) #1\n",
    "    \n",
    "    body = row[2].lower().strip()\n",
    "    tkn_para = []\n",
    "    for para in body.split('<eop>'):\n",
    "        if para and para != ' ':\n",
    "            _para = para + '<eop>'\n",
    "            len_para = len(_para.split())\n",
    "            len_paras.append(len_para) #2\n",
    "            tkn_para.append(_para)\n",
    "    cnt_paras.append(len(tkn_para)) #3\n",
    "    \n",
    "    body_split = []\n",
    "    for tkn in body.split():\n",
    "        if tkn in tkn2tf: tkn2tf[tkn] += 1\n",
    "        else: tkn2tf[tkn] = 1\n",
    "        body_split.append(tkn)\n",
    "    len_bodys.append(len(body_split)) #4\n",
    "            \n",
    "print (n+1), 'Done'\n",
    "print (datetime.datetime.now().time())\n",
    "print ('voca size :', len(tkn2tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44031it [00:00, 658241.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44033 No problem\n",
      "Show top-10 tkn:\n",
      "<eos> : 21225\n",
      "। : 18486\n",
      ", : 11287\n",
      "না : 3522\n",
      "করে : 3274\n",
      "এই : 2523\n",
      "! : 2021\n",
      "? : 1854\n",
      "থেকে : 1721\n",
      "’ : 1613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_token = sorted(tkn2tf.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tkn2idx = {}\n",
    "for idx, (tkn, _) in tqdm(enumerate(sorted_token)):\n",
    "    tkn2idx[tkn] = idx + 2\n",
    "tkn2idx['<unk>'] = 1\n",
    "tkn2idx[''] = 0\n",
    "if len(tkn2idx) == len(tkn2tf)+2:\n",
    "    print (len(tkn2idx), 'No problem')\n",
    "print \n",
    "\n",
    "print ('Show top-10 tkn:')\n",
    "for tkn, freq in sorted_token[:10]:\n",
    "    print (tkn,':',freq)\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dic_mincut.txt\n",
    "with open(\"Output.txt\", \"w\", encoding=\"utf8\") as txt_file:\n",
    "    for tkn, freq in sorted_token:\n",
    "        txt_file.write(tkn + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.txt', 'w', encoding='utf8') as f:\n",
    "    for key in tkn2idx.keys():\n",
    "        f.write(key+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca size : 1654\n"
     ]
    }
   ],
   "source": [
    "tkn2tf_mincut5 = {}\n",
    "for tkn, tf in tkn2tf.items():\n",
    "    if tf < 30:\n",
    "        continue\n",
    "    tkn2tf_mincut5[tkn] = tf\n",
    "print ('voca size :', len(tkn2tf_mincut5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2tf_mincut5['<EOS>'] = tkn2tf_mincut5['<eos>']\n",
    "tkn2tf_mincut5['<EOP>'] = tkn2tf_mincut5['<eop>']\n",
    "\n",
    "del tkn2tf_mincut5['<eos>']\n",
    "del tkn2tf_mincut5['<eop>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_voca = sorted(tkn2tf_mincut5.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing word <EOP>\n",
      "existing word <EOS>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_voca_mincut = []\n",
    "list_voca_mincut.append('')   # PAD\n",
    "list_voca_mincut.append('<UNK>')   # UNK\n",
    "list_voca_mincut.append('<EOS>')   # EOS\n",
    "list_voca_mincut.append('<EOP>')   # EOP\n",
    "\n",
    "for word, idx in sorted_voca:\n",
    "    if word=='<UNK>' or word=='<EOP>' or word=='<EOS>':\n",
    "        print(\"existing word\", word)\n",
    "        continue\n",
    "    else:\n",
    "        list_voca_mincut.append(word)\n",
    "        \n",
    "len(list_voca_mincut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.txt', 'w', encoding='utf8') as f:\n",
    "    for i in range(len(list_voca_mincut)):\n",
    "        f.write(list_voca_mincut[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_voca = {}\n",
    "for voca in list_voca_mincut:\n",
    "    dic_voca[voca] = len(dic_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(dic_voca[''], dic_voca['<UNK>'], dic_voca['<EOS>'], dic_voca['<EOP>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.pkl', 'wb') as f:\n",
    "    cPickle.dump(dic_voca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read voca from dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_voca = []\n",
    "with open(path_processed_data + 'dic_mincutN.txt', 'r', encoding='utf8') as f:\n",
    "    list_voca = f.readlines()\n",
    "    list_voca = [x.strip() for x in list_voca]\n",
    "\n",
    "dic_voca = {}\n",
    "for voca in list_voca:\n",
    "    dic_voca[voca] = len(dic_voca)\n",
    "\n",
    "len(dic_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dic_voca_lower = copy.deepcopy(dic_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_voca_lower['<eos>'] = dic_voca_lower['<EOS>']\n",
    "dic_voca_lower['<eop>'] = dic_voca_lower['<EOP>']\n",
    "\n",
    "del dic_voca_lower['<EOS>']\n",
    "del dic_voca_lower['<EOP>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic_voca_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(dic_voca_lower[''], dic_voca_lower['<UNK>'], dic_voca_lower['<eos>'], dic_voca_lower['<eop>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "data= []\n",
    "with open(path_raw_data + 'train.csv', 'r', encoding='utf8') as f:\n",
    "    data_csv = csv.reader(f)\n",
    "    for row in data_csv:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(data):\n",
    "    print(\"mean\", np.average(data))\n",
    "    print(\"std\", np.std(data))\n",
    "    print(\"max\", np.max(data))\n",
    "    print(\"95.xx coverage\", np.average(data) +  2*np.std(data) )\n",
    "    print(\"99.73 coverage\", np.average(data) +  3*np.std(data) )\n",
    "    print(\"99.95 coverage\", np.average(data) +  3.5*np.std(data) )\n",
    "    print(\"99.99 coverage\", np.average(data) +  4*np.std(data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_len\n",
      "mean 10.455163043478262\n",
      "std 3.7450184993365982\n",
      "max 28\n",
      "95.xx coverage 17.94520004215146\n",
      "99.73 coverage 21.690218541488058\n",
      "99.95 coverage 23.562727791156355\n",
      "99.99 coverage 25.435237040824653\n"
     ]
    }
   ],
   "source": [
    "head = [x[1].strip() for x in data]\n",
    "head_len = [len(x.split()) for x in head]\n",
    "print('head_len')\n",
    "print_info(head_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = [x[2].strip() for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_len\n",
      "mean 512.6399456521739\n",
      "std 353.99887079468215\n",
      "max 3649\n",
      "95.xx coverage 1220.637687241538\n",
      "99.73 coverage 1574.6365580362203\n",
      "99.95 coverage 1751.6359934335615\n",
      "99.99 coverage 1928.6354288309026\n"
     ]
    }
   ],
   "source": [
    "body_len = [len(x.split()) for x in body ]\n",
    "print('body_len')\n",
    "print_info(body_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_len\n",
      "mean 1.5\n",
      "std 0.5\n",
      "max 2\n",
      "95.xx coverage 2.5\n",
      "99.73 coverage 3.0\n",
      "99.95 coverage 3.25\n",
      "99.99 coverage 3.5\n"
     ]
    }
   ],
   "source": [
    "context_len = [len(x.split('<EOP>')) for x in body]\n",
    "print('context_len')\n",
    "print_info(context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_sentence\n",
      "mean 341.4266304347826\n",
      "std 253.72896867414084\n",
      "max 3365\n",
      "95.xx coverage 848.8845677830643\n",
      "99.73 coverage 1102.613536457205\n",
      "99.95 coverage 1229.4780207942756\n",
      "99.99 coverage 1356.3425051313461\n"
     ]
    }
   ],
   "source": [
    "body_sentence = []\n",
    "for sent in body:\n",
    "    sent = sent.split('<EOP>')\n",
    "    body_sentence.extend(sent)\n",
    "body_len = [ len(x.split()) for x in body_sentence ]    \n",
    "print('body_sentence')\n",
    "print_info(body_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# encode to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_length(data, max_len_t, max_len_b):\n",
    "    data_t, data_b = data\n",
    "    \n",
    "    list_zeros = np.zeros(max_len_b, 'int32').tolist()\n",
    "    fl_data_t = []\n",
    "    for datum in data_t:\n",
    "        try:\n",
    "            datum = list(datum)\n",
    "        except:\n",
    "            pass\n",
    "        _len = len(datum)\n",
    "        if _len >= max_len_t:\n",
    "            fl_data_t.append( datum[:max_len_t] )\n",
    "        else:\n",
    "            fl_data_t.append( datum + list_zeros[:(max_len_t-_len)] )\n",
    "            \n",
    "    fl_data_b = []\n",
    "    for datum in data_b:\n",
    "        try:\n",
    "            datum = list(datum)\n",
    "        except:\n",
    "            pass\n",
    "        _len = len(datum)\n",
    "        if _len >= max_len_b:\n",
    "            fl_data_b.append( datum[:max_len_b] )\n",
    "        else:\n",
    "            fl_data_b.append( datum + list_zeros[:(max_len_b-_len)] )\n",
    "    \n",
    "    np_data_t = np.asarray(fl_data_t, dtype='int32')\n",
    "    np_data_b = np.asarray(fl_data_b, dtype='int32')\n",
    "    \n",
    "    data = [np_data_t, np_data_b]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:44:10.431152\n",
      "736 Done\n",
      "2021-04-15T17:44:10.763075\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + 'train.csv', 'r', encoding='utf8'))\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "ids = []\n",
    "heads = []\n",
    "bodys = []\n",
    "labels = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    \n",
    "#     if n <  3000000:\n",
    "#         continue\n",
    "\n",
    "#     if n >=  3000000:\n",
    "#         continue\n",
    "        \n",
    "                \n",
    "    if (n+1) % 10000 == 0: print (n+1,)\n",
    "    \n",
    "    ids.append(row[0])\n",
    "    labels.append((row[3]))\n",
    "    \n",
    "    head = []\n",
    "    for tkn in row[1].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            head.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            head.append(1)\n",
    "            \n",
    "    heads.append(head)\n",
    "    \n",
    "    body = []\n",
    "    for tkn in row[2].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            body.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            body.append(1)\n",
    "            \n",
    "    bodys.append(body)\n",
    "    \n",
    "print (n+1, 'Done')\n",
    "print (datetime.datetime.now().isoformat()) # ~5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:44:19.147959\n",
      "2021-04-15T17:44:19.359396\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "[np_heads, np_bodys] = fit_length([heads, bodys], 25, 2100)\n",
    "print (datetime.datetime.now().isoformat()) # ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:44:25.045399\n",
      "2021-04-15T17:44:25.066343\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/train/train_title.npy'\n",
    "np.save(t_trainpath, np_heads)\n",
    "b_trainpath = path_processed_data + '/train/train_body.npy'\n",
    "np.save(b_trainpath, np_bodys)\n",
    "l_trainpath = path_processed_data + '/train/train_label.npy'\n",
    "np.save(l_trainpath, labels)\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:45:25.780604\n",
      "65 Done\n",
      "2021-04-15T17:45:25.821476\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + 'dev.csv', 'r', encoding='utf8'))\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "ids_dev = []\n",
    "heads_dev = []\n",
    "bodys_dev = []\n",
    "labels_dev = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if (n+1) % 10000 == 0: print (n+1,)\n",
    "    \n",
    "    ids_dev.append(row[0])\n",
    "    labels_dev.append((row[3]))\n",
    "    \n",
    "    head = []\n",
    "    for tkn in row[2].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            head.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            head.append(1)\n",
    "    heads_dev.append(head)\n",
    "    \n",
    "    body = []\n",
    "    for tkn in row[1].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            body.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            body.append(1)\n",
    "    bodys_dev.append(body)\n",
    "    \n",
    "print (n+1, 'Done')\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:45:28.846388\n",
      "2021-04-15T17:45:28.879338\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "[np_heads_dev, np_bodys_dev] = fit_length([heads_dev, bodys_dev], 25, 2100)\n",
    "print (datetime.datetime.now().isoformat()) # ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:45:31.406577\n",
      "2021-04-15T17:45:31.415554\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/dev/dev_title.npy'\n",
    "np.save(t_trainpath, np_heads_dev)\n",
    "b_trainpath = path_processed_data + '/dev/dev_body.npy'\n",
    "np.save(b_trainpath, np_bodys_dev)\n",
    "l_trainpath = path_processed_data + '/dev/dev_label.npy'\n",
    "np.save(l_trainpath, labels_dev)\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:46:27.875728\n",
      "65 Done\n",
      "2021-04-15T17:46:27.928587\n"
     ]
    }
   ],
   "source": [
    "csv_reader = csv.reader(open(path_raw_data + '/test.csv', 'r', encoding='utf8'))\n",
    "\n",
    "print (datetime.datetime.now().isoformat())\n",
    "ids_dev = []\n",
    "heads_dev = []\n",
    "bodys_dev = []\n",
    "labels_dev = []\n",
    "for n, row in enumerate(csv_reader):\n",
    "    if (n+1) % 10000 == 0: print (n+1,)\n",
    "    \n",
    "    ids_dev.append(row[0])\n",
    "    labels_dev.append(row[3])\n",
    "    \n",
    "    head = []\n",
    "    for tkn in row[2].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            head.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            head.append(1)\n",
    "    heads_dev.append(head)\n",
    "    \n",
    "    body = []\n",
    "    for tkn in row[1].lower().strip().split():\n",
    "        if tkn in dic_voca_lower:\n",
    "            body.append(dic_voca_lower[tkn])\n",
    "        else:\n",
    "            body.append(1)\n",
    "    bodys_dev.append(body)\n",
    "    \n",
    "print (n+1, 'Done')\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:46:30.747049\n",
      "2021-04-15T17:46:30.776972\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "[np_heads_dev, np_bodys_dev] = fit_length([heads_dev, bodys_dev], 25, 2100)\n",
    "print (datetime.datetime.now().isoformat()) # ~3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:46:33.119704\n",
      "2021-04-15T17:46:33.129678\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/test/test_title.npy'\n",
    "np.save(t_trainpath, np_heads_dev)\n",
    "b_trainpath = path_processed_data + '/test/test_body.npy'\n",
    "np.save(b_trainpath, np_bodys_dev)\n",
    "l_trainpath = path_processed_data + '/test/test_label.npy'\n",
    "np.save(l_trainpath, labels_dev)\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debugset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-15T17:46:38.117709\n",
      "2021-04-15T17:46:38.126315\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime.now().isoformat())\n",
    "t_trainpath = path_processed_data + '/debug/debug_title.npy'\n",
    "np.save(t_trainpath, np_heads_dev[:200])\n",
    "b_trainpath = path_processed_data + '/debug/debug_body.npy'\n",
    "np.save(b_trainpath, np_bodys_dev[:200])\n",
    "l_trainpath = path_processed_data + '/debug/debug_label.npy'\n",
    "np.save(l_trainpath, labels_dev[:200])\n",
    "print (datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_processed_data + 'dic_mincutN.txt', encoding='utf8') as f:\n",
    "    test_list_voca = f.readlines()\n",
    "    test_list_voca = [x.strip() for x in test_list_voca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Vocab(test_list_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> শেখ <UNK> রহমানের ছোট ভাই শেখ আবু <UNK> ছেলে শেখ <UNK> <UNK> সন্তান শেখ <UNK> <UNK> <UNK> ‘ <UNK> ২ ’ থেকে বাংলাদেশ আওয়ামী\n"
     ]
    }
   ],
   "source": [
    "print(tt.index2sent(np_heads_dev[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
